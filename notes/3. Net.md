# Net

[TOC]

## Net

在有了Blob和Layer之后，Net的设计就很清晰了，它的职责就是把Blob和Layers组织起来，形成一个有向无环图。在给出一个输入之后，可以经过这个网络的各个Layers，最后获得一个输出。

所以，Nets类至少需要一个forward方法，一个backward方法，一个容器记录所有输入blobs，一个容器记录所有输出blobs，一个容器记录layers，一个容器记录所有中间结果的blobs，一个容器记录所有的参数。这些是一个神经网络必须的内容，少了这些里面的任何一个，都无法构造一个真正的计算图。

```cpp
template <typename Dtype>
class Net {
 public:
  // 提供了一组forward方法
  const vector<Blob<Dtype>*>& Forward(Dtype* loss = NULL);
  Dtype ForwardFromTo(int start, int end);
  Dtype ForwardFrom(int start);
  Dtype ForwardTo(int end);
  // 提供了一组backward方法
  void Backward();
  void BackwardFromTo(int start, int end);
  void BackwardFrom(int start);
  void BackwardTo(int end);
 protected:
  vector<shared_ptr<Layer<Dtype>>> layers_;  // vector容器记录所有layer
  vector<shared_ptr<Blob<Dtype>>> blobs_;  // vector容器记录所有中间结果的blobs
  vector<vector<Blob<Dtype>*>> bottom_vecs_;  // vector容器存储所有的layer的输入，可以看到是嵌套的vector，第一层的vector size是layer的数目，第二层就是每个layer的输入blobs
  vector<vector<Blob<Dtype>*>> top_vecs_;  // vector容器存储所有layer的输出。
  vector<shared_ptr<Blob<Dtype>>> params_;  // 所有参数
}
```

## Implementation

### Net的构造和析构

构造函数核心逻辑在Net::Init()中，从Net的声明中可以看到，它接收一个NetParameter参数，那么理论上NetParameter中就应该有上面说的Net需要的所有内容。

`void Net<Dtype>::Init(const NetParameter& in_param)`

让我们来看一下NetParameter中的主要内容

```cpp
message NetParameter {
  optional string name = 1;  // 网络的名字，例如VGG
  optional bool force_backward = 5 [default = false];  // 执行时候强制网络中的所有layer都执行backward，如果是false则根据网络结构和学习率自动推断
  optional NetState state = 6;  // 网络状态，具体内容如下：phase、level、stage
  optional bool debug_info = 7 [default = false];  // 是否需要打印调试信息
  repeated LayerParameter layer = 100;  // 网络中所有layer的内容
}
```

```cpp
message NetState {
  optional Phase phase = 1 [default = TEST];  // Train/Test
  optional int32 level = 2 [default = 0];
  repeated string stage = 3;
}
```

接下来看一下Init()的核心内容，因为Net是串联的Layer

```cpp
template <typename Dtype>
void Net<Dtype>::Init(const NetParameter& in_param) {
  NetParameter filtered_param;
  FilterNet(in_param, &filtered_param);

  NetParameter param;
  InsertSplits(filtered_param, &param);

  NetParameter param;
  bottom_vecs_.resize(param.layer_size());
  top_vecs_.resize(param.layer_size());
  bottom_id_vecs_.resize(param.layer_size());
  param_id_vecs_.resize(param.layer_size());
  top_id_vecs_.resize(param.layer_size());
  bottom_need_backward_.resize(param.layer_size());

  // 逐层进行初始化
  for (int layer_id = 0; layer_id < param.layer_size(); ++layer_id) {
    const LayerParameter& layer_param = param.layer(layer_id);  // 从net param中获取layer param
    layers_.push_back(LayerRegistry<Dtype>::CreateLayer(layer_param));  // 使用工厂返回对应的layer对象，添加到layer_中由net统一记录

    // 添加输入
    for (int bottom_id = 0; bottom_id < layer_param.bottom_size(); ++bottom_id) {
      const int blob_id = AppendBottom(param, layer_id, bottom_id, &available_blobs, &blob_name_to_idx);
      need_backward |= blob_need_backward_[blob_id];
    }

    // 添加输出
    int num_top = layer_param.top_size();
    for (int top_id = 0; top_id < num_top; ++top_id) {
      AppendTop(param, layer_id, top_id, &available_blobs, &blob_name_to_idx);
      // 如果当前输入blob不是中间计算结果而是网络输入，则统一记录到net_input_blobs中
      if (layer_param.type() == "Input") {
        const int blob_id = blobs_.size() - 1;
        net_input_blob_indices_.push_back(blob_id);
        net_input_blobs_.push_back(blobs_[blob_id].get());
      }
    }

    // 对每层进行初始化
    layers_[layer_id]->SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);

    // 遍历每一个layer的输出，设置相应的loss权重和更新需要的总内存空间
    for (int top_id = 0; top_id < top_vecs_[layer_id].size(); ++top_id) {
      if (blob_loss_weights_.size() <= top_id_vecs_[layer_id][top_id]) {
        blob_loss_weights_.resize(top_id_vecs_[layer_id][top_id] + 1, Dtype(0));
      }
      blob_loss_weights_[top_id_vecs_[layer_id][top_id]] = layer->loss(top_id);
      memory_used_ += top_vecs_[layer_id][top_id]->count();
    }

    // 逐一设置当前layer的参数是否需要计算梯度
    const int param_size = layer_param.param_size();
    const int num_param_blobs = layers_[layer_id]->blobs().size();
    ParamSpec default_param_spec;
    for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
      const ParamSpec* param_spec = (param_id < param_size) ?
          &layer_param.param(param_id) : &default_param_spec;
      const bool param_need_backward = param_spec->lr_mult() != 0;
      need_backward |= param_need_backward;
      layers_[layer_id]->set_param_propagate_down(param_id,
                                                  param_need_backward);
    }

    // 逐一记录当前参数
    for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
      AppendParam(param, layer_id, param_id);
    }
  }
}
```

对于每一层输入数据，有`AppendBottom`来负责添加

```cpp
template <typename Dtype>
int Net<Dtype>::AppendBottom(const NetParameter& param, const int layer_id,
    const int bottom_id, set<string>* available_blobs,
    map<string, int>* blob_name_to_idx) {
  const LayerParameter& layer_param = param.layer(layer_id);
  const string& blob_name = layer_param.bottom(bottom_id);

  const int blob_id = (*blob_name_to_idx)[blob_name];

  // 根据layer param的记录，添加到net的管理容器中
  bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());
  bottom_id_vecs_[layer_id].push_back(blob_id);
  available_blobs->erase(blob_name);

  // 记录该blob是否参数后续的反向传播
  bool need_backward = blob_need_backward_[blob_id];
  if (layer_param.propagate_down_size() > 0) {
    need_backward = layer_param.propagate_down(bottom_id);
  }
  bottom_need_backward_[layer_id].push_back(need_backward);
  return blob_id;
}
```

对于每一层输出数据，有`AppendTop`来负责添加

```cpp
template <typename Dtype>
void Net<Dtype>::AppendTop(const NetParameter& param, const int layer_id,
                           const int top_id, set<string>* available_blobs,
                           map<string, int>* blob_name_to_idx) {
  shared_ptr<LayerParameter> layer_param(new LayerParameter(param.layer(layer_id)));

  // 判断是否需要原地计算
  if (blob_name_to_idx && layer_param->bottom_size() > top_id &&
      blob_name == layer_param->bottom(top_id)) {
    // 需要原地计算：在输入中可以找到这个输入blob的名字，就是用的同一个blob
    top_vecs_[layer_id].push_back(blobs_[(*blob_name_to_idx)[blob_name]].get());
    top_id_vecs_[layer_id].push_back((*blob_name_to_idx)[blob_name]);
  } else if (blob_name_to_idx && blob_name_to_idx->find(blob_name) != blob_name_to_idx->end()) {
    // 不需要原地计算，但是有发现blob记录中已经有这个blob了，就报错，比如可能在网络定义时对于不同layer使用了相同过的blob
    // 直接抛出异常
  } else {
    // 不需要原地计算：有独立的输入和输出
    shared_ptr<Blob<Dtype> > blob_pointer(new Blob<Dtype>());
    const int blob_id = blobs_.size();
    blobs_.push_back(blob_pointer);
    blob_names_.push_back(blob_name);
    blob_need_backward_.push_back(false);
    if (blob_name_to_idx) { (*blob_name_to_idx)[blob_name] = blob_id; }
    top_id_vecs_[layer_id].push_back(blob_id);
    top_vecs_[layer_id].push_back(blob_pointer.get());
  }
  if (available_blobs) { available_blobs->insert(blob_name); }
}
```

其实，上面整个Init过程就是根据Net param和layer param进行一系列需要的信息的填充，所以就不扣很细节的内容了。

### Forward

虽然forward提供了一组方法，但是实际上它们是同源的，我们只来看最终的一个方法`ForwardFromTo`，当start是0，end是`layer.size() - 1`时，就是一个完整的从头到尾的forward过程了。

```cpp
template <typename Dtype>
Dtype Net<Dtype>::ForwardFromTo(int start, int end) {
  Dtype loss = 0;
  for (int i = start; i <= end; ++i) {
    for (int c = 0; c < before_forward_.size(); ++c) {
      before_forward_[c]->run(i);  // 先执行一遍before callback
    }
    Dtype layer_loss = layers_[i]->Forward(bottom_vecs_[i], top_vecs_[i]);  // 调用layer的实际计算方法执行
    loss += layer_loss;  // 当这个layer不是最终输出的layer时，layer_loss是0，这里的逻辑其实在layer中已经看过了
    if (debug_info_) { ForwardDebugInfo(i); }
    for (int c = 0; c < after_forward_.size(); ++c) {
      after_forward_[c]->run(i);  // 再执行一遍after callback
    }
  }
  return loss;
}
```

### Backward
